# å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªåŸºäº Apache Airflow çš„æ•°æ®è´¨é‡ç›‘æ§é¡¹ç›®ï¼Œä¸»è¦ç”¨äºè‚¡ç¥¨æ•°æ®çš„è‡ªåŠ¨åŒ–é‡‡é›†å’Œè´¨é‡ç›‘æ§ã€‚

**ğŸ¯ é¡¹ç›®ç›®æ ‡**: é€šè¿‡å®é™…ä¸šåŠ¡åœºæ™¯ï¼Œå…¨é¢å±•ç¤º Apache Airflow çš„æ ¸å¿ƒåŠŸèƒ½ç‰¹æ€§ï¼ŒåŒ…æ‹¬è‡ªå®šä¹‰ç»„ä»¶å¼€å‘ã€ä»»åŠ¡ç¼–æ’ã€æ•°æ®è´¨é‡ç›‘æ§ç­‰ã€‚

---

## ğŸš€ ç¯å¢ƒè¦æ±‚

- Python 3.11.5
- Apache Airflow 2.10.5
- Docker & Docker Compose (æ¨è)
- MySQL 8.0
- PostgreSQL 13

---

## âš¡ å¿«é€Ÿå¯åŠ¨

### æ–¹å¼ä¸€: Docker Compose å¯åŠ¨ (æ¨è)

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <repository-url>
cd RiskDataQuality-Airflow

# 2. ä¸€é”®å¯åŠ¨æ‰€æœ‰æœåŠ¡
./scripts/start.sh

# æˆ–æ‰‹åŠ¨å¯åŠ¨
docker-compose up -d

# 3. ç­‰å¾…æœåŠ¡å¯åŠ¨ (çº¦ 1-2 åˆ†é’Ÿ)
docker-compose logs -f

# 4. è®¿é—® Airflow Web UI
# http://localhost:8080
# ç”¨æˆ·å: airflow
# å¯†ç : airflow
```

**åœæ­¢æœåŠ¡**:
```bash
./scripts/stop.sh
# æˆ–
docker-compose down
```

---

### æ–¹å¼äºŒ: æœ¬åœ°ç›´æ¥å¯åŠ¨

#### 1. åˆ›å»º Conda ç¯å¢ƒ
```bash
conda create -n AirFlow python=3.11.5
conda activate AirFlow
```

#### 2. å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

æˆ–æ‰‹åŠ¨å®‰è£…:
```bash
pip install apache-airflow
pip install mysql-connector-python
pip install yfinance
pip install pytest
pip install apache-airflow-providers-mysql
```

#### 3. åˆå§‹åŒ– Airflow
```bash
# åˆå§‹åŒ–æ•°æ®åº“
airflow db init

# åˆ›å»ºç®¡ç†å‘˜ç”¨æˆ·
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin
```

#### 4. å¯åŠ¨æœåŠ¡
```bash
# å¯åŠ¨ web æœåŠ¡å™¨ (ç«¯å£ 8080)
airflow webserver --port 8080

# æ–°å¼€ç»ˆç«¯çª—å£ï¼Œå¯åŠ¨è°ƒåº¦å™¨
airflow scheduler
```

---

## ğŸ”§ æ•°æ®åº“é…ç½®

### MySQL é…ç½®

é¡¹ç›®éœ€è¦ MySQL æ•°æ®åº“æ”¯æŒ:
- **æ•°æ®åº“å**: `DataFlow_DB`
- **è¡¨å**: `stock_prices` (è‡ªåŠ¨åˆ›å»º)
- **è¿æ¥ ID**: `MySQL` (åœ¨ Airflow ä¸­é…ç½®)

#### è¡¨ç»“æ„
```sql
CREATE TABLE stock_prices (
    symbol VARCHAR(10),
    date DATE,
    open FLOAT,
    high FLOAT,
    low FLOAT,
    close FLOAT,
    volume BIGINT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (symbol, date)
);
```

#### é…ç½® Airflow è¿æ¥

**Docker ç¯å¢ƒ** (åœ¨å®¹å™¨å†…æ‰§è¡Œ):
```bash
docker-compose exec airflow-webserver airflow connections add 'MySQL' \
  --conn-type 'mysql' \
  --conn-host 'mysql' \
  --conn-login 'airflow' \
  --conn-password 'airflow' \
  --conn-schema 'DataFlow_DB' \
  --conn-port 3306
```

**æœ¬åœ°ç¯å¢ƒ**:
```bash
airflow connections add 'MySQL' \
  --conn-type 'mysql' \
  --conn-host 'localhost' \
  --conn-login 'root' \
  --conn-password 'your_password' \
  --conn-schema 'DataFlow_DB' \
  --conn-port 3306
```

æˆ–åœ¨ Airflow Web UI ä¸­é…ç½®:
1. è®¿é—® Admin â†’ Connections
2. ç‚¹å‡» "+" æ·»åŠ æ–°è¿æ¥
3. å¡«å†™è¿æ¥ä¿¡æ¯

---

## ğŸ“Š è®¿é—®ä¿¡æ¯

### Docker ç¯å¢ƒ
- **Airflow Web UI**: http://localhost:8080
- **ç”¨æˆ·å**: `airflow`
- **å¯†ç **: `airflow`
- **PostgreSQL** (Airflow å…ƒæ•°æ®): `localhost:5432`
- **MySQL** (ä¸šåŠ¡æ•°æ®): `localhost:3306`

### æ•°æ®åº“è¿æ¥
```bash
# PostgreSQL
psql -h localhost -p 5432 -U airflow -d airflow

# MySQL
mysql -h 127.0.0.1 -P 3306 -u airflow -p
# å¯†ç : airflow
```

---

## ğŸ® å¸¸ç”¨å‘½ä»¤

### DAG ç®¡ç†
```bash
# æŸ¥çœ‹ DAG åˆ—è¡¨
airflow dags list

# æµ‹è¯• DAG è¯­æ³•
airflow dags test stock_data_pipeline

# æ‰‹åŠ¨è§¦å‘ DAG
airflow dags trigger stock_data_pipeline

# æ£€æŸ¥ DAG çŠ¶æ€
airflow dags state stock_data_pipeline $(date +"%Y-%m-%d")
```

### ä»»åŠ¡æ“ä½œ
```bash
# æµ‹è¯•å•ä¸ªä»»åŠ¡
airflow tasks test stock_data_pipeline download_stock_data $(date +"%Y-%m-%d")

# æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—
airflow tasks log stock_data_pipeline download_stock_data $(date +"%Y-%m-%d")
```

### å˜é‡ç®¡ç†
```bash
# è®¾ç½®è‚¡ç¥¨ä»£ç å˜é‡
airflow variables set stock_symbols '["AAPL", "MSFT", "GOOGL", "AMZN"]'

# æŸ¥çœ‹å˜é‡
airflow variables get stock_symbols

# åˆ—å‡ºæ‰€æœ‰å˜é‡
airflow variables list
```

### Docker ç®¡ç†
```bash
# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ‰€æœ‰æ—¥å¿—
docker-compose logs -f

# æŸ¥çœ‹ç‰¹å®šæœåŠ¡æ—¥å¿—
docker-compose logs -f webserver
docker-compose logs -f scheduler

# é‡å¯æœåŠ¡
docker-compose restart

# åœæ­¢å¹¶åˆ é™¤æ•°æ®å·
docker-compose down -v

# è¿›å…¥å®¹å™¨
docker-compose exec airflow-webserver bash
docker-compose exec mysql bash
```

---

## ğŸ§ª æµ‹è¯•

### è¿è¡Œæµ‹è¯•
```bash
# è¿è¡Œæ‰€æœ‰å•å…ƒæµ‹è¯•
python -m pytest tests/unit/ -v

# è¿è¡Œé›†æˆæµ‹è¯•
python -m pytest tests/integration/ -v

# è¿è¡Œç‰¹å®šæµ‹è¯•æ–‡ä»¶
python -m pytest tests/unit/test_stock_data_pipeline.py -v

# ä½¿ç”¨æµ‹è¯•è„šæœ¬
./scripts/run_tests.sh
```

---

## âš ï¸ æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

#### 1. Airflow æ— æ³•å¯åŠ¨
```bash
# æ£€æŸ¥ç«¯å£å ç”¨
lsof -i :8080

# æ¸…ç†å¹¶é‡æ–°åˆå§‹åŒ–
airflow db reset
airflow db init
```

#### 2. DAG æœªæ˜¾ç¤º
- æ£€æŸ¥ DAG æ–‡ä»¶è¯­æ³•: `python dags/stock_data_pipeline.py`
- æŸ¥çœ‹ Scheduler æ—¥å¿—: `docker-compose logs -f scheduler`
- ç¡®è®¤ DAG æ–‡ä»¶åœ¨æ­£ç¡®ç›®å½•: `dags/`

#### 3. æ•°æ®åº“è¿æ¥å¤±è´¥
- æ£€æŸ¥ MySQL æœåŠ¡çŠ¶æ€: `docker-compose ps mysql`
- éªŒè¯è¿æ¥é…ç½®: Airflow UI â†’ Admin â†’ Connections
- æµ‹è¯•è¿æ¥: `airflow connections test MySQL`

#### 4. ä¾èµ–å®‰è£…å¤±è´¥
```bash
# Docker ç¯å¢ƒé‡æ–°æ„å»º
docker-compose down
docker-compose build --no-cache
docker-compose up -d

# æœ¬åœ°ç¯å¢ƒ
pip install --upgrade pip
pip install -r requirements.txt --force-reinstall
```

---

## ğŸ“š ä¸‹ä¸€æ­¥

- ğŸ“– é˜…è¯» [é¡¹ç›®æ¶æ„è®¾è®¡](ARCHITECTURE.md)
- ğŸ—ï¸ æŸ¥çœ‹ [é¡¹ç›®ç»“æ„è¯´æ˜](PROJECT_STRUCTURE.md)
- ğŸ’» å­¦ä¹  [å¼€å‘æŒ‡å—](DEVELOPMENT.md)
- ğŸ“… äº†è§£ [å¼€å‘è®¡åˆ’](ROADMAP.md)

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Request!

## ğŸ“„ è®¸å¯è¯

MIT License
